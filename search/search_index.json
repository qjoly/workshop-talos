{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome aboard Welcome to the Talos Workshop documentation site. Here you'll find all the information you need to set up and run Talos workshops using our Proxmox VE Terraform stack. Environment Before diving in, ensure you have the following prerequisites in place: A user with sufficient privileges on the Proxmox VE host to create VMs (clone a template, start/stop VMs) talosctl installed locally to manage Talos nodes (otherwise, follow the Talos installation guide ) kubectl installed locally to interact with the Kubernetes clusters running on Talos nodes (installation instructions can be found here ) Fell free to ask for help or open issues if you run into any trouble! Ready to launch an actual cluster? Follow the hands-on walkthrough in Create My First Talos Cluster . For those who don't know Kubernetes yet, I recommend checking out the \"What is Kubernetes?\" page to get a basic understanding of the concepts involved.","title":"Home"},{"location":"#welcome-aboard","text":"Welcome to the Talos Workshop documentation site. Here you'll find all the information you need to set up and run Talos workshops using our Proxmox VE Terraform stack.","title":"Welcome aboard"},{"location":"#environment","text":"Before diving in, ensure you have the following prerequisites in place: A user with sufficient privileges on the Proxmox VE host to create VMs (clone a template, start/stop VMs) talosctl installed locally to manage Talos nodes (otherwise, follow the Talos installation guide ) kubectl installed locally to interact with the Kubernetes clusters running on Talos nodes (installation instructions can be found here ) Fell free to ask for help or open issues if you run into any trouble! Ready to launch an actual cluster? Follow the hands-on walkthrough in Create My First Talos Cluster . For those who don't know Kubernetes yet, I recommend checking out the \"What is Kubernetes?\" page to get a basic understanding of the concepts involved.","title":"Environment"},{"location":"create-first-cluster/","text":"Create My First Talos Cluster This guide walks you through building a minimal Talos-based Kubernetes cluster on top of the Proxmox VE environment prepared by this Terraform project. You will provision a single control-plane VM, add a worker, bootstrap Kubernetes, and confirm connectivity with kubectl . Prerequisites Terraform stack already applied so the Talos ISO is uploaded and workshop users/pools exist. Template VM (default VMID 100 ) containing the Talos install media and ISO. talosctl v1.11+ and kubectl installed on your workstation. Network access from your workstation to the Talos nodes (SSH is not required, but HTTPS/50000 must be reachable). 1. Generate machine configurations Use talosctl gen secrets to create a shared secret for the cluster. Use talosctl gen config to create control-plane and worker configurations. Replace the IPs with the ones you plan to assign to your VMs. talosctl gen config workshop-cluster https://10.0.0.10:6443 # Optional: tweak the generated YAML files (controlplane.yaml, worker.yaml) # to set static routes, registries, or kubelet flags. You will get two more files: controlplane.yaml and worker.yaml . 2. Clone the Talos template In Proxmox VE, select the template VM (VMID 100 by default). Click Clone , choose a new VMID (for example 201 ), and target your assigned pool/storage. Set the CPU, memory, and disk sizes to match the workshop instructions (typically 2 vCPU / 4 GiB RAM / 20 GiB disk for control planes). Repeat for each additional node (create at least one worker VM such as VMID 211 ). 3. Apply Talos machine configs Once the cloned VM boots from the Talos ISO it will display its management IP on the serial console. Use that IP to push the appropriate configuration file. # Apply the control-plane config CONTROL_PLANE_IP=10.0.0.11 talosctl apply-config \\ --insecure \\ --nodes ${CONTROL_PLANE_IP} \\ --file cluster-config/controlplane.yaml # Apply the worker config talosctl apply-config \\ --insecure \\ --nodes 10.0.0.21 \\ --file cluster-config/worker.yaml Wait until both nodes reboot and reach the Ready state ( talosctl health --endpoints ${CONTROL_PLANE_IP} ). Configure the talosconfig endpoint to point to your control-plane node: talosctl config endpoint ${CONTROL_PLANE_IP} talosctl config nodes ${CONTROL_PLANE_IP} 4. Bootstrap the cluster With at least one control-plane node configured, run the bootstrap command once: talosctl bootstrap --endpoints ${CONTROL_PLANE_IP} This initializes etcd, the Kubernetes API server, and the remaining control-plane components. 5. Retrieve kubeconfig and verify talosctl kubeconfig export KUBECONFIG=./kubeconfig kubectl get nodes You should see both the control-plane and worker nodes in Ready status. Deploy a sample workload (for example, the Deployment manifest ) to confirm the cluster works end-to-end. Troubleshooting tips talosctl get machineconfig helps confirm whether the VM picked up the intended YAML. Use talosctl logs kubelet -n ${NODE_IP} if pods get stuck in ContainerCreating . Once everything looks good you have a baseline Talos cluster. Use snapshots or Terraform state to reset quickly between workshop iterations. Next steps Now that your Talos cluster is up and running, here are some recommended next steps: Explore Talos machine configuration options to customize networking, storage,and security settings. Deploy more complex Kubernetes workloads using kubectl and practice scaling,updating, and monitoring Is it finished? No, there are still lots of things to improve. Secrets are contained in plain text inside the generated machine config files. How to customize a Talos node to add features like VPN, monitoring agents, or logging? Commands are scoped to a single node at a time. Managing multiple nodes could be tedious. Overrides to the base Talos config (for example, adding static routes) require manual YAML edits. Let's address these pain points in nexts chapters.","title":"Create your first cluster"},{"location":"create-first-cluster/#create-my-first-talos-cluster","text":"This guide walks you through building a minimal Talos-based Kubernetes cluster on top of the Proxmox VE environment prepared by this Terraform project. You will provision a single control-plane VM, add a worker, bootstrap Kubernetes, and confirm connectivity with kubectl .","title":"Create My First Talos Cluster"},{"location":"create-first-cluster/#prerequisites","text":"Terraform stack already applied so the Talos ISO is uploaded and workshop users/pools exist. Template VM (default VMID 100 ) containing the Talos install media and ISO. talosctl v1.11+ and kubectl installed on your workstation. Network access from your workstation to the Talos nodes (SSH is not required, but HTTPS/50000 must be reachable).","title":"Prerequisites"},{"location":"create-first-cluster/#1-generate-machine-configurations","text":"Use talosctl gen secrets to create a shared secret for the cluster. Use talosctl gen config to create control-plane and worker configurations. Replace the IPs with the ones you plan to assign to your VMs. talosctl gen config workshop-cluster https://10.0.0.10:6443 # Optional: tweak the generated YAML files (controlplane.yaml, worker.yaml) # to set static routes, registries, or kubelet flags. You will get two more files: controlplane.yaml and worker.yaml .","title":"1. Generate machine configurations"},{"location":"create-first-cluster/#2-clone-the-talos-template","text":"In Proxmox VE, select the template VM (VMID 100 by default). Click Clone , choose a new VMID (for example 201 ), and target your assigned pool/storage. Set the CPU, memory, and disk sizes to match the workshop instructions (typically 2 vCPU / 4 GiB RAM / 20 GiB disk for control planes). Repeat for each additional node (create at least one worker VM such as VMID 211 ).","title":"2. Clone the Talos template"},{"location":"create-first-cluster/#3-apply-talos-machine-configs","text":"Once the cloned VM boots from the Talos ISO it will display its management IP on the serial console. Use that IP to push the appropriate configuration file. # Apply the control-plane config CONTROL_PLANE_IP=10.0.0.11 talosctl apply-config \\ --insecure \\ --nodes ${CONTROL_PLANE_IP} \\ --file cluster-config/controlplane.yaml # Apply the worker config talosctl apply-config \\ --insecure \\ --nodes 10.0.0.21 \\ --file cluster-config/worker.yaml Wait until both nodes reboot and reach the Ready state ( talosctl health --endpoints ${CONTROL_PLANE_IP} ). Configure the talosconfig endpoint to point to your control-plane node: talosctl config endpoint ${CONTROL_PLANE_IP} talosctl config nodes ${CONTROL_PLANE_IP}","title":"3. Apply Talos machine configs"},{"location":"create-first-cluster/#4-bootstrap-the-cluster","text":"With at least one control-plane node configured, run the bootstrap command once: talosctl bootstrap --endpoints ${CONTROL_PLANE_IP} This initializes etcd, the Kubernetes API server, and the remaining control-plane components.","title":"4. Bootstrap the cluster"},{"location":"create-first-cluster/#5-retrieve-kubeconfig-and-verify","text":"talosctl kubeconfig export KUBECONFIG=./kubeconfig kubectl get nodes You should see both the control-plane and worker nodes in Ready status. Deploy a sample workload (for example, the Deployment manifest ) to confirm the cluster works end-to-end.","title":"5. Retrieve kubeconfig and verify"},{"location":"create-first-cluster/#troubleshooting-tips","text":"talosctl get machineconfig helps confirm whether the VM picked up the intended YAML. Use talosctl logs kubelet -n ${NODE_IP} if pods get stuck in ContainerCreating . Once everything looks good you have a baseline Talos cluster. Use snapshots or Terraform state to reset quickly between workshop iterations.","title":"Troubleshooting tips"},{"location":"create-first-cluster/#next-steps","text":"Now that your Talos cluster is up and running, here are some recommended next steps: Explore Talos machine configuration options to customize networking, storage,and security settings. Deploy more complex Kubernetes workloads using kubectl and practice scaling,updating, and monitoring Is it finished? No, there are still lots of things to improve. Secrets are contained in plain text inside the generated machine config files. How to customize a Talos node to add features like VPN, monitoring agents, or logging? Commands are scoped to a single node at a time. Managing multiple nodes could be tedious. Overrides to the base Talos config (for example, adding static routes) require manual YAML edits. Let's address these pain points in nexts chapters.","title":"Next steps"},{"location":"extensions/","text":"Now that you're familiar with the core features, let's explore how to enhance your experience using extensions. Extensions allow you to add new functionalities and customize your workflow. Why Install Extensions? Since Talos is a minimalist OS, it doesn\u2019t include all the tools you might expect from a traditional OS. If you want to install an EDR agent (like CrowdStrike), a driver for an NVIDIA card, or any other program that cannot run in Kubernetes (including static pods ), you\u2019ll need to use extensions. There are two ways to install an extension on Talos: By using a custom Talos image that already includes the extension; Or by specifying an OCI image containing the extension to be installed. Each method corresponds to a different use case. The custom image is useful when the extension is necessary for Talos in maintenance mode (when it is waiting for its configuration). A common example of this case could be a driver (e.g. a RAID card) whose presence is mandatory for Talos to detect the device (if it doesn\u2019t find a disk, it cannot be installed). The other method, which involves installing the node using a specific OCI image(installer), is useful when the extension needs to be active while the node is already running. For example, a RuntimeClass to launch MicroVMs in Kubernetes, a Tailscale VPN\u2026 Of course, it is possible to use both methods simultaneously. In the case of the RAID driver, it must be present in the base Talos image for the disk to be recognized, and then reinstalled via the OCI image so that the extension is retained after the OS installation. Thus, it is needed both before and after installation. An important point to keep in mind: if you specify in the Talos configuration an image providing an extension (in the machine.install.image field or during an upgrade), it will replace the extensions already present. In summary, here\u2019s what you need to remember: If you install a custom Talos image and use the default OCI image (ghcr.io/siderolabs/installer:v1.x.x), the extension will be retained after the node installation. If you install a custom Talos image and use a different OCI image (in the configuration or during an update), the extension will not be retained. How do we install an extension? Let\u2019s take a concrete example. I often use Proxmox in my labs. It\u2019s a hypervisor I appreciate for its flexibility and simplicity of use. Notably, it has a handy feature: displaying the IPs of VMs in the web interface. To achieve this, you need to install an agent on each VM so they can report this information. Thus, we need to install the qemu-guest-agent on our nodes. Let\u2019s see how to do this. Using a Custom Talos Image Installing a Custom Talos Image The simplest method to create this image, which already contains the extension, is to use Factory. This site allows you to fill out a form to create an image tailored to your needs (architecture, kernel args, Talos version, extensions). You will be presented with a page asking you to check the various extensions you want to install. Simply check qemu-guest-agent and validate. Depending on how you want to install your machine, you will have the choice between: Downloading the ISO image; Downloading the disk image (raw); Using a PXE script. Your image request is associated with an ID ( ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515 in my case) that you can use to retrieve the image at any time. It is also possible to script the image generation using the Factory API. When you finish filling out the form, you get a summary of your request \u201cas code\u201d in YAML: customization: systemExtensions: officialExtensions: - siderolabs/qemu-guest-agent You can also use this summary to re-generate the image like this: $ yq eval -o=json customization.yaml > customization.json # Convert it to JSON $ curl -s -X POST https://factory.talos.dev/schematics \\ -H \"Content-Type: application/json\" \\ -d @customization.json {\"id\":\"ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515\"} From there, you can easily integrate the image generation into a pipeline or automation script\u2026 Or continue using the Factory web interface, it\u2019s up to you \ud83d\ude04. By downloading the raw disk image, you can then use it to create your VMs in Proxmox or any other hypervisor of your choice with the pre-installed extension. Otherwise, you can also use image OCI\u202fto upgrade an existing Talos installation with the extension. You can use it like below: schematicid=376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba talosctl upgrade -i factory.talos.dev/installer/${schematicid}:v1.12.1 --nodes <node-ip> Now you can customize your Talos image with the extensions you need! Want to deep dive into extensions ? Check out my article Customizing Talos with Extensions where I explore how to create your own extensions.","title":"Add features by creating extensions"},{"location":"extensions/#why-install-extensions","text":"Since Talos is a minimalist OS, it doesn\u2019t include all the tools you might expect from a traditional OS. If you want to install an EDR agent (like CrowdStrike), a driver for an NVIDIA card, or any other program that cannot run in Kubernetes (including static pods ), you\u2019ll need to use extensions. There are two ways to install an extension on Talos: By using a custom Talos image that already includes the extension; Or by specifying an OCI image containing the extension to be installed. Each method corresponds to a different use case. The custom image is useful when the extension is necessary for Talos in maintenance mode (when it is waiting for its configuration). A common example of this case could be a driver (e.g. a RAID card) whose presence is mandatory for Talos to detect the device (if it doesn\u2019t find a disk, it cannot be installed). The other method, which involves installing the node using a specific OCI image(installer), is useful when the extension needs to be active while the node is already running. For example, a RuntimeClass to launch MicroVMs in Kubernetes, a Tailscale VPN\u2026 Of course, it is possible to use both methods simultaneously. In the case of the RAID driver, it must be present in the base Talos image for the disk to be recognized, and then reinstalled via the OCI image so that the extension is retained after the OS installation. Thus, it is needed both before and after installation. An important point to keep in mind: if you specify in the Talos configuration an image providing an extension (in the machine.install.image field or during an upgrade), it will replace the extensions already present. In summary, here\u2019s what you need to remember: If you install a custom Talos image and use the default OCI image (ghcr.io/siderolabs/installer:v1.x.x), the extension will be retained after the node installation. If you install a custom Talos image and use a different OCI image (in the configuration or during an update), the extension will not be retained.","title":"Why Install Extensions?"},{"location":"extensions/#how-do-we-install-an-extension","text":"Let\u2019s take a concrete example. I often use Proxmox in my labs. It\u2019s a hypervisor I appreciate for its flexibility and simplicity of use. Notably, it has a handy feature: displaying the IPs of VMs in the web interface. To achieve this, you need to install an agent on each VM so they can report this information. Thus, we need to install the qemu-guest-agent on our nodes. Let\u2019s see how to do this.","title":"How do we install an extension?"},{"location":"extensions/#using-a-custom-talos-image","text":"Installing a Custom Talos Image The simplest method to create this image, which already contains the extension, is to use Factory. This site allows you to fill out a form to create an image tailored to your needs (architecture, kernel args, Talos version, extensions). You will be presented with a page asking you to check the various extensions you want to install. Simply check qemu-guest-agent and validate. Depending on how you want to install your machine, you will have the choice between: Downloading the ISO image; Downloading the disk image (raw); Using a PXE script. Your image request is associated with an ID ( ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515 in my case) that you can use to retrieve the image at any time. It is also possible to script the image generation using the Factory API. When you finish filling out the form, you get a summary of your request \u201cas code\u201d in YAML: customization: systemExtensions: officialExtensions: - siderolabs/qemu-guest-agent You can also use this summary to re-generate the image like this: $ yq eval -o=json customization.yaml > customization.json # Convert it to JSON $ curl -s -X POST https://factory.talos.dev/schematics \\ -H \"Content-Type: application/json\" \\ -d @customization.json {\"id\":\"ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515\"} From there, you can easily integrate the image generation into a pipeline or automation script\u2026 Or continue using the Factory web interface, it\u2019s up to you \ud83d\ude04. By downloading the raw disk image, you can then use it to create your VMs in Proxmox or any other hypervisor of your choice with the pre-installed extension. Otherwise, you can also use image OCI\u202fto upgrade an existing Talos installation with the extension. You can use it like below: schematicid=376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba talosctl upgrade -i factory.talos.dev/installer/${schematicid}:v1.12.1 --nodes <node-ip> Now you can customize your Talos image with the extensions you need! Want to deep dive into extensions ? Check out my article Customizing Talos with Extensions where I explore how to create your own extensions.","title":"Using a Custom Talos Image"},{"location":"hidden-ref/","text":"Documentation Pages Getting Started Create My First Talos Cluster - Step-by-step guide to build your first minimal Talos-based Kubernetes cluster on Proxmox VE Core Concepts What is Kubernetes? - Introduction to Kubernetes, its core concepts, and why it's the de facto standard for container orchestration Talos Linux - Learn about Talos, the minimalist immutable Linux distribution designed specifically for running Kubernetes clusters Tools & Automation Terraform with Talos and Helm - Automate deployment and management of Talos clusters using Infrastructure as Code Talhelper - Simplify Talos management with this command-line assistant for generating and applying configurations Advanced Topics Working with Extensions - Enhance Talos with extensions to add drivers, agents, and other functionalities not included in the minimal OS","title":"Hidden pages"},{"location":"hidden-ref/#documentation-pages","text":"","title":"Documentation Pages"},{"location":"hidden-ref/#getting-started","text":"Create My First Talos Cluster - Step-by-step guide to build your first minimal Talos-based Kubernetes cluster on Proxmox VE","title":"Getting Started"},{"location":"hidden-ref/#core-concepts","text":"What is Kubernetes? - Introduction to Kubernetes, its core concepts, and why it's the de facto standard for container orchestration Talos Linux - Learn about Talos, the minimalist immutable Linux distribution designed specifically for running Kubernetes clusters","title":"Core Concepts"},{"location":"hidden-ref/#tools-automation","text":"Terraform with Talos and Helm - Automate deployment and management of Talos clusters using Infrastructure as Code Talhelper - Simplify Talos management with this command-line assistant for generating and applying configurations","title":"Tools &amp; Automation"},{"location":"hidden-ref/#advanced-topics","text":"Working with Extensions - Enhance Talos with extensions to add drivers, agents, and other functionalities not included in the minimal OS","title":"Advanced Topics"},{"location":"kubernetes/","text":"What is Kubernetes? Kubernetes (often abbreviated as K8s) is an open-source platform that automates deploying, scaling, and operating containerized applications. It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). With Kubernetes you can run applications across clusters of machines while letting the control plane handle scheduling, failover, and service discovery. Why Kubernetes? Kubernetes has become the de facto standard for container orchestration due to its powerful abstractions and extensibility. It enables developers to focus on building applications without worrying about the underlying infrastructure. Key benefits include: Scalability : Easily scale applications up or down based on demand. Self-healing : Automatically restarts failed containers, replaces and reschedules them when nodes die. Service discovery and load balancing : Exposes containers using DNS names or IP Auditing and monitoring : Integrates with logging and monitoring tools to provide insights into application performance. Note that (warning, this is a hot take) Kubernetes can match to many use cases, even without the need for a full-blown microservices architecture or high availability, you can use it to simply manage containerized workloads on a single node cluster. Core concepts Control plane vs. worker nodes Control plane : hosts components like the API server, scheduler, and controllers. It makes global decisions and exposes the Kubernetes API. Worker nodes : run your application workloads inside containers. Each node hosts a kubelet (agent) and a container runtime (containerd, CRI-O, etc.). Control plane components can run on dedicated nodes or be co-located with worker nodes in smaller clusters. Pods Pods are the smallest deployable unit in Kubernetes. A pod usually wraps a single container, but multiple tightly coupled containers can run inside one pod when they must share storage or networking. apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx:latest ports: - containerPort: 80 Services Services provide a stable virtual IP (ClusterIP) or endpoint for a set of pods. They abstract away pod IP churn and can also expose workloads outside the cluster via NodePort, LoadBalancer, or Ingress resources. apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 type: ClusterIP Deployments Deployments manage the lifecycle of replicated pods. You declare the desired number of replicas and Kubernetes ensures the actual state matches (self-healing). Rolling updates and rollbacks are handled automatically. apiVersion: apps/v1 kind: Deployment metadata: name: web-frontend spec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: nginx image: nginx:1.27 ports: - containerPort: 80 env: - name: APP_MESSAGE valueFrom: configMapKeyRef: name: web-config key: welcome-message ConfigMaps and Secrets ConfigMaps : store non-sensitive configuration data so you can decouple config from container images. Secrets : same pattern as ConfigMaps but designed for sensitive data (credentials, certificates). They are base64 encoded and can be encrypted at rest. apiVersion: v1 kind: ConfigMap metadata: name: web-config data: welcome-message: \"Hello Talos workshop!\" feature-flags: | enableCaching=true showBetaBanner=false How to deploy Kubernetes? Kubeadm: official tool to bootstrap a minimal cluster. Managed services: EKS (AWS), GKE (GCP), AKS (Azure) offer fully managed Kubernetes clusters. Ansible with Kubespray: for more control over cluster configuration. K3s/K0s: lightweight Kubernetes distribution for edge and IoT. Talos Linux: purpose-built OS for running Kubernetes clusters with an API-driven approach. Why Kubernetes matters for the Talos workshop Talos Linux provides a minimal, API-driven operating system for running Kubernetes control planes and worker nodes. Understanding Kubernetes basics helps you: Validate that Talos nodes joined your cluster and became Ready. Deploy Talos machine configs that reference Kubernetes resources (manifests, patches). Use kubectl to inspect pods, services, and events when troubleshooting. Scale your workshop workloads (add replicas, expose services) without touching the underlying VMs. Next steps If you are brand new to Kubernetes, practice the following commands once your Talos cluster is up: kubectl get nodes kubectl get pods -A kubectl describe pod <pod-name> -n <namespace> These commands let you confirm node status, list workloads across namespaces, and inspect pod-level events/logs. Armed with these fundamentals, you will navigate the rest of the workshop much more confidently.","title":"What's Kubernetes?"},{"location":"kubernetes/#what-is-kubernetes","text":"Kubernetes (often abbreviated as K8s) is an open-source platform that automates deploying, scaling, and operating containerized applications. It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF). With Kubernetes you can run applications across clusters of machines while letting the control plane handle scheduling, failover, and service discovery.","title":"What is Kubernetes?"},{"location":"kubernetes/#why-kubernetes","text":"Kubernetes has become the de facto standard for container orchestration due to its powerful abstractions and extensibility. It enables developers to focus on building applications without worrying about the underlying infrastructure. Key benefits include: Scalability : Easily scale applications up or down based on demand. Self-healing : Automatically restarts failed containers, replaces and reschedules them when nodes die. Service discovery and load balancing : Exposes containers using DNS names or IP Auditing and monitoring : Integrates with logging and monitoring tools to provide insights into application performance. Note that (warning, this is a hot take) Kubernetes can match to many use cases, even without the need for a full-blown microservices architecture or high availability, you can use it to simply manage containerized workloads on a single node cluster.","title":"Why Kubernetes?"},{"location":"kubernetes/#core-concepts","text":"","title":"Core concepts"},{"location":"kubernetes/#control-plane-vs-worker-nodes","text":"Control plane : hosts components like the API server, scheduler, and controllers. It makes global decisions and exposes the Kubernetes API. Worker nodes : run your application workloads inside containers. Each node hosts a kubelet (agent) and a container runtime (containerd, CRI-O, etc.). Control plane components can run on dedicated nodes or be co-located with worker nodes in smaller clusters.","title":"Control plane vs. worker nodes"},{"location":"kubernetes/#pods","text":"Pods are the smallest deployable unit in Kubernetes. A pod usually wraps a single container, but multiple tightly coupled containers can run inside one pod when they must share storage or networking. apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: nginx:latest ports: - containerPort: 80","title":"Pods"},{"location":"kubernetes/#services","text":"Services provide a stable virtual IP (ClusterIP) or endpoint for a set of pods. They abstract away pod IP churn and can also expose workloads outside the cluster via NodePort, LoadBalancer, or Ingress resources. apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 type: ClusterIP","title":"Services"},{"location":"kubernetes/#deployments","text":"Deployments manage the lifecycle of replicated pods. You declare the desired number of replicas and Kubernetes ensures the actual state matches (self-healing). Rolling updates and rollbacks are handled automatically. apiVersion: apps/v1 kind: Deployment metadata: name: web-frontend spec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: nginx image: nginx:1.27 ports: - containerPort: 80 env: - name: APP_MESSAGE valueFrom: configMapKeyRef: name: web-config key: welcome-message","title":"Deployments"},{"location":"kubernetes/#configmaps-and-secrets","text":"ConfigMaps : store non-sensitive configuration data so you can decouple config from container images. Secrets : same pattern as ConfigMaps but designed for sensitive data (credentials, certificates). They are base64 encoded and can be encrypted at rest. apiVersion: v1 kind: ConfigMap metadata: name: web-config data: welcome-message: \"Hello Talos workshop!\" feature-flags: | enableCaching=true showBetaBanner=false","title":"ConfigMaps and Secrets"},{"location":"kubernetes/#how-to-deploy-kubernetes","text":"Kubeadm: official tool to bootstrap a minimal cluster. Managed services: EKS (AWS), GKE (GCP), AKS (Azure) offer fully managed Kubernetes clusters. Ansible with Kubespray: for more control over cluster configuration. K3s/K0s: lightweight Kubernetes distribution for edge and IoT. Talos Linux: purpose-built OS for running Kubernetes clusters with an API-driven approach.","title":"How to deploy Kubernetes?"},{"location":"kubernetes/#why-kubernetes-matters-for-the-talos-workshop","text":"Talos Linux provides a minimal, API-driven operating system for running Kubernetes control planes and worker nodes. Understanding Kubernetes basics helps you: Validate that Talos nodes joined your cluster and became Ready. Deploy Talos machine configs that reference Kubernetes resources (manifests, patches). Use kubectl to inspect pods, services, and events when troubleshooting. Scale your workshop workloads (add replicas, expose services) without touching the underlying VMs.","title":"Why Kubernetes matters for the Talos workshop"},{"location":"kubernetes/#next-steps","text":"If you are brand new to Kubernetes, practice the following commands once your Talos cluster is up: kubectl get nodes kubectl get pods -A kubectl describe pod <pod-name> -n <namespace> These commands let you confirm node status, list workloads across namespaces, and inspect pod-level events/logs. Armed with these fundamentals, you will navigate the rest of the workshop much more confidently.","title":"Next steps"},{"location":"talhelper/","text":"Talhelper: your Talos command-line assistant Talhelper is a command-line tool that simplifies common Talos management tasks, such as generating machine configuration files and applying them to nodes. It abstracts away some of the complexity of using talosctl directly. With it, you can also separate cluster configuration and secret files , making it easier to manage and share configurations without exposing sensitive data. Installation Check the documentation at budimanjojo.github.io to install talhelper on your system. Usage Talhelper provides several commands to streamline Talos operations but you still have to resort to talosctl for certain tasks like bootstrapping the cluster. Generate the cluster secret: talhelper gensecret > talsecret.yaml Generate machine configuration files for control-plane and worker nodes: # talconfig.yaml --- clusterName: talos-cluster talosVersion: v1.12.1 kubernetesVersion: v1.34.1 endpoint: https://192.168.1.101:6443 allowSchedulingOnMasters: true patches: - |- - op: replace path: /machine/network/kubespan value: enabled: true controlPlane: patches: - |- - op: add path: /machine/kubelet/extraArgs value: feature-gates: ServerSideApply=true nodes: - hostname: cp-1 ipAddress: 192.168.1.101 controlPlane: true installDisk: /dev/sda - hostname: cp-2 ipAddress: 192.168.1.102 controlPlane: true installDisk: /dev/sda - hostname: cp-3 ipAddress: 192.168.1.103 controlPlane: true installDisk: /dev/sda - hostname: worker-1 ipAddress: 192.168.1.114 controlPlane: false installDisk: /dev/sda schematic: customization: extraKernelArgs: - net.ifnames=0 This example talconfig.yaml file defines a Talos cluster with three control-plane nodes and one worker node, along with specific configurations. You can customize it further based on your requirements. Notice that many parameters can be set using patches, which allows you to modify the generated configuration for a specific node type without changing the base configuration. Once you're done, you can generate the machine configuration files: talhelper genconfig This will create the necessary Talos machine configuration files for each node in the cluster, based on the specifications in talconfig.yaml . Each node will have its own configuration file.","title":"Use talhelper to optimize the workflow"},{"location":"talhelper/#talhelper-your-talos-command-line-assistant","text":"Talhelper is a command-line tool that simplifies common Talos management tasks, such as generating machine configuration files and applying them to nodes. It abstracts away some of the complexity of using talosctl directly. With it, you can also separate cluster configuration and secret files , making it easier to manage and share configurations without exposing sensitive data.","title":"Talhelper: your Talos command-line assistant"},{"location":"talhelper/#installation","text":"Check the documentation at budimanjojo.github.io to install talhelper on your system.","title":"Installation"},{"location":"talhelper/#usage","text":"Talhelper provides several commands to streamline Talos operations but you still have to resort to talosctl for certain tasks like bootstrapping the cluster. Generate the cluster secret: talhelper gensecret > talsecret.yaml Generate machine configuration files for control-plane and worker nodes: # talconfig.yaml --- clusterName: talos-cluster talosVersion: v1.12.1 kubernetesVersion: v1.34.1 endpoint: https://192.168.1.101:6443 allowSchedulingOnMasters: true patches: - |- - op: replace path: /machine/network/kubespan value: enabled: true controlPlane: patches: - |- - op: add path: /machine/kubelet/extraArgs value: feature-gates: ServerSideApply=true nodes: - hostname: cp-1 ipAddress: 192.168.1.101 controlPlane: true installDisk: /dev/sda - hostname: cp-2 ipAddress: 192.168.1.102 controlPlane: true installDisk: /dev/sda - hostname: cp-3 ipAddress: 192.168.1.103 controlPlane: true installDisk: /dev/sda - hostname: worker-1 ipAddress: 192.168.1.114 controlPlane: false installDisk: /dev/sda schematic: customization: extraKernelArgs: - net.ifnames=0 This example talconfig.yaml file defines a Talos cluster with three control-plane nodes and one worker node, along with specific configurations. You can customize it further based on your requirements. Notice that many parameters can be set using patches, which allows you to modify the generated configuration for a specific node type without changing the base configuration. Once you're done, you can generate the machine configuration files: talhelper genconfig This will create the necessary Talos machine configuration files for each node in the cluster, based on the specifications in talconfig.yaml . Each node will have its own configuration file.","title":"Usage"},{"location":"talos/","text":"Talos Linux Talos is a minimalist, immutable Linux distribution designed specifically to run Kubernetes clusters in production. Instead of behaving like a traditional Linux install, it works more like firmware for your cluster: the entire operating system is managed declaratively, upgraded atomically, and exposed through secure APIs rather than SSH shells. The API-driven model deserves special attention because it changes how platform teams interact with their infrastructure. Instead of juggling SSH keys, mutable scripts, and ad-hoc procedures, every lifecycle operation funnels through a consistent RPC surface that accepts signed requests, returns machine-readable responses, and can be wired directly into CI/CD pipelines. This enables GitOps-style workflows for the operating system itself, makes auditing straightforward, and eliminates the class of mistakes that come from human drift on individual servers. See more about the components of Talos architecture in the architecture overview . Why no shell or SSH? Copy/pasted from talos.dev FAQ Since Talos is fully API-driven, all maintenance and debugging operations are possible via the OS API. We would like for Talos users to start thinking about what a \u201cmachine\u201d is in the context of a Kubernetes cluster. That is, that a Kubernetes cluster can be thought of as one massive machine, and the nodes are merely additional, undifferentiated resources. We don\u2019t want humans to focus on the nodes, but rather on the machine that is the Kubernetes cluster. Should an issue arise at the node level, talosctl should provide the necessary tooling to assist in the identification, debugging, and remediation of the issue. However, the API is based on the Principle of Least Privilege, and exposes only a limited set of methods. We envision Talos being a great place for the application of control theory in order to provide a self-healing platform. Unlike traditional Linux distributions, Talos does not include a package manager, shell, or SSH server. You'll find alternative of linux command line tools via talosctl commands. For example: https://docs.siderolabs.com/talos/v1.12/learn-more/talos-for-linux-admins Why It Is Cool Upgrades feel as safe as rolling out a new container image because the OS swap is atomic and reversible. Security defaults stay tight: there is no package manager to exploit, no shell to break into, and TPM plus secure-boot fits right in. Consistency shines since dev, staging, and prod all boot from the same artifact, which shrinks debugging time and audit scope. Because Talos ships only what Kubernetes needs, it boots fast, sips resources, and drastically lowers the CVE surface area. Automation fans get first-class tooling through TalHelper, Terraform, Cluster API, and anything else that can spit out the declarative config. Reliability Guarantees Atomicity and immutability are the backbone of those guarantees. Atomicity means every OS upgrade happens as an all-or-nothing transaction: Talos writes the new image to an alternate partition, verifies it, and only then flips a pointer, so failure mid-upgrade never leaves a machine in limbo. Immutability means that image is read-only at runtime, so no one can sneak in a stray package or tweak; the only way to change the system is to supply a new declared state. Together they ensure that every node either runs the exact version you intended or it cleanly rolls back to the previous one, with no in-between snowflakes. Typical Workflow Start by writing the cluster topology and node roles in YAML. Feed that spec into TalHelper, Terraform, or your own pipelines to render Talos machine configs along with any sealed secrets. Boot the machines from ISO, PXE, or cloud images; as soon as they discover their config they bootstrap themselves and knit together the control plane. Afterwards every lifecycle task\u2014reboots, upgrades, certificate rotation\u2014flows through the Talos API, keeping operations scripted and auditable. When To Reach For Talos Talos shines when you are self-hosting Kubernetes on bare metal, edge gear, or homelab rigs and want consistency without babysitting. It is a strong fit for regulated environments where immutable infrastructure, GitOps workflows, and auditable change histories are non-negotiable. Teams tired of snowflake servers or converged infrastructure drift can treat Talos as a clean slate that scales from a single box to a rack without changing operating procedures. Comparison Snapshot Feature Talos Traditional Linux Node Access model API-driven, no SSH SSH + config management Upgrades Atomic, immutable image Package-level, imperative Default services Kubernetes essentials only General-purpose OS stack Drift control Declarative configs Depends on tooling Security posture Hardened, minimal surface Varies by distro and hardening Learn More Head to https://www.talos.dev for the full documentation, then follow the walkthrough in docs/create-first-cluster.md to get a cluster running with this repository. Talos turns the OS into a reliable building block so you can focus on the Kubernetes workloads that matter.","title":"Talos Linux"},{"location":"talos/#talos-linux","text":"Talos is a minimalist, immutable Linux distribution designed specifically to run Kubernetes clusters in production. Instead of behaving like a traditional Linux install, it works more like firmware for your cluster: the entire operating system is managed declaratively, upgraded atomically, and exposed through secure APIs rather than SSH shells. The API-driven model deserves special attention because it changes how platform teams interact with their infrastructure. Instead of juggling SSH keys, mutable scripts, and ad-hoc procedures, every lifecycle operation funnels through a consistent RPC surface that accepts signed requests, returns machine-readable responses, and can be wired directly into CI/CD pipelines. This enables GitOps-style workflows for the operating system itself, makes auditing straightforward, and eliminates the class of mistakes that come from human drift on individual servers. See more about the components of Talos architecture in the architecture overview .","title":"Talos Linux"},{"location":"talos/#why-no-shell-or-ssh","text":"Copy/pasted from talos.dev FAQ Since Talos is fully API-driven, all maintenance and debugging operations are possible via the OS API. We would like for Talos users to start thinking about what a \u201cmachine\u201d is in the context of a Kubernetes cluster. That is, that a Kubernetes cluster can be thought of as one massive machine, and the nodes are merely additional, undifferentiated resources. We don\u2019t want humans to focus on the nodes, but rather on the machine that is the Kubernetes cluster. Should an issue arise at the node level, talosctl should provide the necessary tooling to assist in the identification, debugging, and remediation of the issue. However, the API is based on the Principle of Least Privilege, and exposes only a limited set of methods. We envision Talos being a great place for the application of control theory in order to provide a self-healing platform. Unlike traditional Linux distributions, Talos does not include a package manager, shell, or SSH server. You'll find alternative of linux command line tools via talosctl commands. For example: https://docs.siderolabs.com/talos/v1.12/learn-more/talos-for-linux-admins","title":"Why no shell or SSH?"},{"location":"talos/#why-it-is-cool","text":"Upgrades feel as safe as rolling out a new container image because the OS swap is atomic and reversible. Security defaults stay tight: there is no package manager to exploit, no shell to break into, and TPM plus secure-boot fits right in. Consistency shines since dev, staging, and prod all boot from the same artifact, which shrinks debugging time and audit scope. Because Talos ships only what Kubernetes needs, it boots fast, sips resources, and drastically lowers the CVE surface area. Automation fans get first-class tooling through TalHelper, Terraform, Cluster API, and anything else that can spit out the declarative config.","title":"Why It Is Cool"},{"location":"talos/#reliability-guarantees","text":"Atomicity and immutability are the backbone of those guarantees. Atomicity means every OS upgrade happens as an all-or-nothing transaction: Talos writes the new image to an alternate partition, verifies it, and only then flips a pointer, so failure mid-upgrade never leaves a machine in limbo. Immutability means that image is read-only at runtime, so no one can sneak in a stray package or tweak; the only way to change the system is to supply a new declared state. Together they ensure that every node either runs the exact version you intended or it cleanly rolls back to the previous one, with no in-between snowflakes.","title":"Reliability Guarantees"},{"location":"talos/#typical-workflow","text":"Start by writing the cluster topology and node roles in YAML. Feed that spec into TalHelper, Terraform, or your own pipelines to render Talos machine configs along with any sealed secrets. Boot the machines from ISO, PXE, or cloud images; as soon as they discover their config they bootstrap themselves and knit together the control plane. Afterwards every lifecycle task\u2014reboots, upgrades, certificate rotation\u2014flows through the Talos API, keeping operations scripted and auditable.","title":"Typical Workflow"},{"location":"talos/#when-to-reach-for-talos","text":"Talos shines when you are self-hosting Kubernetes on bare metal, edge gear, or homelab rigs and want consistency without babysitting. It is a strong fit for regulated environments where immutable infrastructure, GitOps workflows, and auditable change histories are non-negotiable. Teams tired of snowflake servers or converged infrastructure drift can treat Talos as a clean slate that scales from a single box to a rack without changing operating procedures.","title":"When To Reach For Talos"},{"location":"talos/#comparison-snapshot","text":"Feature Talos Traditional Linux Node Access model API-driven, no SSH SSH + config management Upgrades Atomic, immutable image Package-level, imperative Default services Kubernetes essentials only General-purpose OS stack Drift control Declarative configs Depends on tooling Security posture Hardened, minimal surface Varies by distro and hardening","title":"Comparison Snapshot"},{"location":"talos/#learn-more","text":"Head to https://www.talos.dev for the full documentation, then follow the walkthrough in docs/create-first-cluster.md to get a cluster running with this repository. Talos turns the OS into a reliable building block so you can focus on the Kubernetes workloads that matter.","title":"Learn More"},{"location":"terraform/","text":"Terraform with Talos and Helm This guide shows you how to use Terraform to automate the deployment and management of Talos Linux clusters with integrated Helm application deployment. Overview Terraform is an Infrastructure as Code (IaC) tool that allows you to define and provision infrastructure through declarative configuration files. When combined with Talos Linux and Helm, it provides a complete automation solution from cluster creation to application deployment. Terraform Providers Talos Provider The Talos provider allows you to manage Talos Linux clusters declaratively. Version : 0.10.1 Provider : siderolabs/talos Capabilities: - Generate machine configurations - Bootstrap clusters - Manage cluster secrets - Apply configuration patches - Generate kubeconfig and talosconfig Helm Provider The Helm provider enables automated application deployment to Kubernetes. Version : 3.0.2 Provider : hashicorp/helm Features: - Deploy Helm charts - Manage releases - Configure chart values - Automatic authentication via kubeconfig Project Structure The workshop's Terraform configuration for Talos is in the tofu/talos/ directory: tofu/talos/ \u251c\u2500\u2500 provider.tf # Talos and Helm providers configuration \u251c\u2500\u2500 vars.tf # Cluster variables (IP, name) \u2514\u2500\u2500 main.tf # Complete cluster lifecycle and Helm deployments Getting Started Before deploying a Talos cluster with Terraform, ensure you have: Talos nodes ready : VMs or bare metal machines with Talos Linux booted Network configuration : IP addresses assigned to your nodes Terraform installed : Version 1.10.0 or higher Step 1: Provider Configuration The provider.tf file configures both the Talos and Helm providers: terraform { required_version = \">= 1.10.0\" required_providers { talos = { source = \"siderolabs/talos\" version = \"0.10.1\" } helm = { source = \"hashicorp/helm\" version = \"3.0.2\" } } } provider \"helm\" { kubernetes = { host = \"https://${var.cp_ip}:6443\" client_certificate = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).users[0].user.client-certificate-data) client_key = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).users[0].user.client-key-data) cluster_ca_certificate = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).clusters[0].cluster.certificate-authority-data) } } Key Points: - Uses Terraform >= 1.10.0 - Talos provider version 0.10.1 - Helm provider 3.0.2 configured with Talos-generated kubeconfig - Helm automatically extracts certificates from the kubeconfig Step 2: Define Variables The vars.tf file defines cluster parameters: variable \"cp_ip\" { type = string default = \"192.168.0.222\" description = \"Control plane IP address\" } variable \"cluster_name\" { type = string default = \"my-cluster\" description = \"Name of the Kubernetes cluster\" } Step 3: Complete Cluster Deployment The main.tf file implements the full cluster lifecycle: # 1. Generate machine secrets resource \"talos_machine_secrets\" \"talos\" { } # 2. Generate controller configuration data \"talos_machine_configuration\" \"controller\" { cluster_name = var.cluster_name cluster_endpoint = \"https://${var.cp_ip}:6443\" machine_type = \"controlplane\" machine_secrets = talos_machine_secrets.talos.machine_secrets config_patches = [ yamlencode({ cluster = { allowSchedulingOnControlPlanes = true } }), ] } # 3. Generate talosconfig data \"talos_client_configuration\" \"talosconfig\" { cluster_name = var.cluster_name client_configuration = talos_machine_secrets.talos.client_configuration endpoints = [var.cp_ip] } # 4. Apply configuration to control plane resource \"talos_machine_configuration_apply\" \"cp_config_apply\" { client_configuration = talos_machine_secrets.talos.client_configuration machine_configuration_input = data.talos_machine_configuration.controller.machine_configuration count = 1 node = var.cp_ip } # 5. Bootstrap the cluster resource \"talos_machine_bootstrap\" \"bootstrap\" { depends_on = [ talos_machine_configuration_apply.cp_config_apply ] client_configuration = talos_machine_secrets.talos.client_configuration node = var.cp_ip } # 6. Generate kubeconfig resource \"talos_cluster_kubeconfig\" \"kubeconfig\" { depends_on = [ talos_machine_bootstrap.bootstrap ] client_configuration = talos_machine_secrets.talos.client_configuration node = var.cp_ip } # 7. Deploy application with Helm resource \"helm_release\" \"nginx\" { depends_on = [ talos_cluster_kubeconfig.kubeconfig ] name = \"nginx\" repository = \"https://charts.bitnami.com/bitnami\" chart = \"nginx\" namespace = \"default\" set = [ { name = \"service.type\" value = \"NodePort\" } ] } # Outputs output \"talosconfig\" { value = data.talos_client_configuration.talosconfig.talos_config sensitive = true } output \"kubeconfig\" { value = talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw sensitive = true } Understanding the Workflow Machine Secrets : Generate cryptographic secrets for the cluster Machine Configuration : Create control plane configuration with scheduling enabled Talosconfig : Generate client configuration for talosctl commands Apply Configuration : Push configuration to the control plane node Bootstrap : Initialize the Kubernetes cluster Kubeconfig : Generate Kubernetes client configuration Helm Deployment : Automatically deploy an NGINX application Deploying the Example cd tofu/talos/ # Initialize Terraform terraform init # Review the plan terraform plan # Deploy the cluster terraform apply # Extract kubeconfig terraform output -raw kubeconfig > ~/.kube/config # Extract talosconfig terraform output -raw talosconfig > ~/.talos/config # Verify the cluster kubectl get nodes kubectl get pods -A","title":"Terraform to deploy Talos everywhere"},{"location":"terraform/#terraform-with-talos-and-helm","text":"This guide shows you how to use Terraform to automate the deployment and management of Talos Linux clusters with integrated Helm application deployment.","title":"Terraform with Talos and Helm"},{"location":"terraform/#overview","text":"Terraform is an Infrastructure as Code (IaC) tool that allows you to define and provision infrastructure through declarative configuration files. When combined with Talos Linux and Helm, it provides a complete automation solution from cluster creation to application deployment.","title":"Overview"},{"location":"terraform/#terraform-providers","text":"","title":"Terraform Providers"},{"location":"terraform/#talos-provider","text":"The Talos provider allows you to manage Talos Linux clusters declaratively. Version : 0.10.1 Provider : siderolabs/talos Capabilities: - Generate machine configurations - Bootstrap clusters - Manage cluster secrets - Apply configuration patches - Generate kubeconfig and talosconfig","title":"Talos Provider"},{"location":"terraform/#helm-provider","text":"The Helm provider enables automated application deployment to Kubernetes. Version : 3.0.2 Provider : hashicorp/helm Features: - Deploy Helm charts - Manage releases - Configure chart values - Automatic authentication via kubeconfig","title":"Helm Provider"},{"location":"terraform/#project-structure","text":"The workshop's Terraform configuration for Talos is in the tofu/talos/ directory: tofu/talos/ \u251c\u2500\u2500 provider.tf # Talos and Helm providers configuration \u251c\u2500\u2500 vars.tf # Cluster variables (IP, name) \u2514\u2500\u2500 main.tf # Complete cluster lifecycle and Helm deployments","title":"Project Structure"},{"location":"terraform/#getting-started","text":"Before deploying a Talos cluster with Terraform, ensure you have: Talos nodes ready : VMs or bare metal machines with Talos Linux booted Network configuration : IP addresses assigned to your nodes Terraform installed : Version 1.10.0 or higher","title":"Getting Started"},{"location":"terraform/#step-1-provider-configuration","text":"The provider.tf file configures both the Talos and Helm providers: terraform { required_version = \">= 1.10.0\" required_providers { talos = { source = \"siderolabs/talos\" version = \"0.10.1\" } helm = { source = \"hashicorp/helm\" version = \"3.0.2\" } } } provider \"helm\" { kubernetes = { host = \"https://${var.cp_ip}:6443\" client_certificate = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).users[0].user.client-certificate-data) client_key = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).users[0].user.client-key-data) cluster_ca_certificate = base64decode(yamldecode(talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw).clusters[0].cluster.certificate-authority-data) } } Key Points: - Uses Terraform >= 1.10.0 - Talos provider version 0.10.1 - Helm provider 3.0.2 configured with Talos-generated kubeconfig - Helm automatically extracts certificates from the kubeconfig","title":"Step 1: Provider Configuration"},{"location":"terraform/#step-2-define-variables","text":"The vars.tf file defines cluster parameters: variable \"cp_ip\" { type = string default = \"192.168.0.222\" description = \"Control plane IP address\" } variable \"cluster_name\" { type = string default = \"my-cluster\" description = \"Name of the Kubernetes cluster\" }","title":"Step 2: Define Variables"},{"location":"terraform/#step-3-complete-cluster-deployment","text":"The main.tf file implements the full cluster lifecycle: # 1. Generate machine secrets resource \"talos_machine_secrets\" \"talos\" { } # 2. Generate controller configuration data \"talos_machine_configuration\" \"controller\" { cluster_name = var.cluster_name cluster_endpoint = \"https://${var.cp_ip}:6443\" machine_type = \"controlplane\" machine_secrets = talos_machine_secrets.talos.machine_secrets config_patches = [ yamlencode({ cluster = { allowSchedulingOnControlPlanes = true } }), ] } # 3. Generate talosconfig data \"talos_client_configuration\" \"talosconfig\" { cluster_name = var.cluster_name client_configuration = talos_machine_secrets.talos.client_configuration endpoints = [var.cp_ip] } # 4. Apply configuration to control plane resource \"talos_machine_configuration_apply\" \"cp_config_apply\" { client_configuration = talos_machine_secrets.talos.client_configuration machine_configuration_input = data.talos_machine_configuration.controller.machine_configuration count = 1 node = var.cp_ip } # 5. Bootstrap the cluster resource \"talos_machine_bootstrap\" \"bootstrap\" { depends_on = [ talos_machine_configuration_apply.cp_config_apply ] client_configuration = talos_machine_secrets.talos.client_configuration node = var.cp_ip } # 6. Generate kubeconfig resource \"talos_cluster_kubeconfig\" \"kubeconfig\" { depends_on = [ talos_machine_bootstrap.bootstrap ] client_configuration = talos_machine_secrets.talos.client_configuration node = var.cp_ip } # 7. Deploy application with Helm resource \"helm_release\" \"nginx\" { depends_on = [ talos_cluster_kubeconfig.kubeconfig ] name = \"nginx\" repository = \"https://charts.bitnami.com/bitnami\" chart = \"nginx\" namespace = \"default\" set = [ { name = \"service.type\" value = \"NodePort\" } ] } # Outputs output \"talosconfig\" { value = data.talos_client_configuration.talosconfig.talos_config sensitive = true } output \"kubeconfig\" { value = talos_cluster_kubeconfig.kubeconfig.kubeconfig_raw sensitive = true }","title":"Step 3: Complete Cluster Deployment"},{"location":"terraform/#understanding-the-workflow","text":"Machine Secrets : Generate cryptographic secrets for the cluster Machine Configuration : Create control plane configuration with scheduling enabled Talosconfig : Generate client configuration for talosctl commands Apply Configuration : Push configuration to the control plane node Bootstrap : Initialize the Kubernetes cluster Kubeconfig : Generate Kubernetes client configuration Helm Deployment : Automatically deploy an NGINX application","title":"Understanding the Workflow"},{"location":"terraform/#deploying-the-example","text":"cd tofu/talos/ # Initialize Terraform terraform init # Review the plan terraform plan # Deploy the cluster terraform apply # Extract kubeconfig terraform output -raw kubeconfig > ~/.kube/config # Extract talosconfig terraform output -raw talosconfig > ~/.talos/config # Verify the cluster kubectl get nodes kubectl get pods -A","title":"Deploying the Example"}]}